{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694b68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bff3b6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUN , BUN/Creatinine Ratio\n",
      "0.8774417368026577\n",
      "BUN/Creatinine Ratio , BUN\n",
      "0.8774417368026577\n"
     ]
    }
   ],
   "source": [
    "from PlotSpinalCordDatafile_withSlidingWindow import test as sodiumInterpVals\n",
    "from PlotSpinalCordDatafile_withSlidingWindow import interpTimes, times\n",
    "\n",
    "from PlotSpinalCordDatafile_withSlidingWindow import dataArr\n",
    "from PlotSpinalCordDatafile_withSlidingWindow import slidingData \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f297d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "method = 'dopri5'   #\n",
    "learning_rate = 1e-3\n",
    "data_size = 26\n",
    "# train_len = data_size\n",
    "train_len = int(data_size * 0.8)\n",
    "test_len = int((data_size - train_len))\n",
    "# valid_len = data_size - train_len - test_len\n",
    "batch_size = 8\n",
    "batch_time = test_len-1 # Must be in [1, test_len-1]\n",
    "niters = 1000 # epoch/ number of iteration steps\n",
    "input_dim = 2\n",
    "output_dim = 1\n",
    "hidden_dim1 = 60\n",
    "hidden_dim2 = 60\n",
    "data_index = 0 # Biomarker index\n",
    "gpu = 0\n",
    "adjoint = False\n",
    "print(train_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca5e0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if adjoint:\n",
    "    from torchdiffeq import odeint_adjoint as odeint\n",
    "    # print('adjoint is:odeint_adjoint')\n",
    "else:\n",
    "    from torchdiffeq import odeint\n",
    "    # print('ajoint is odeint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11439859",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0894dab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[141. 140. 138. 139. 138. 141. 139. 141. 142. 139. 142. 139. 142. 142.\n",
      " 140. 139. 139. 141. 137. 141. 140. 141. 144. 145. 147. 143.]\n",
      "[1217737. 1016080.  874834.  834544.  774086.  733753.  693271.  632994.\n",
      "  566848.  473272.  432863.  392559.  372264.  352148.  321956.  301766.\n",
      "  271567.  239827.  170814.  130406.   98711.   71492.    7721.    6179.\n",
      "    5585.       0.]\n"
     ]
    }
   ],
   "source": [
    "print(dataArr[0])\n",
    "print(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a8552a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.           3.87847222   4.29097222   5.36180556  49.64722222\n",
      "  68.54930556  90.55972222 118.62083333 166.54652778 188.58819444\n",
      " 209.55972222 223.58055556 244.54722222 258.51666667 272.61041667\n",
      " 300.59930556 328.66111111 393.64444444 439.57916667 481.43819444\n",
      " 509.55069444 537.55972222 579.54444444 607.52361111 705.61111111\n",
      " 845.65069444]\n"
     ]
    }
   ],
   "source": [
    "times = (np.flip(times))/24/60\n",
    "print(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "962a6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data):\n",
    "    noStr = np.flip(data) #define noStr to be the sodium \n",
    "    window_size = 3\n",
    "    moving_averages = [] #array to keep the new \"long wave\" values\n",
    "\n",
    "    ##Checking to see if the variables are defined properly\n",
    "    #print(len(noStr))\n",
    "    #print(len(np.flip(times)))\n",
    "\n",
    "    # Loop through the array t o\n",
    "    #consider every window of size 3\n",
    "    i = 0\n",
    "    while  i < len(noStr)-window_size+1:\n",
    "\n",
    "        # Calculate the average of current window\n",
    "        window_average = round(np.sum(noStr[i:i+window_size]) / window_size, 2)\n",
    "\n",
    "        # Store the average of current\n",
    "        # window in moving average list\n",
    "        moving_averages.append(window_average)\n",
    "\n",
    "        # Shift window to right by one position\n",
    "        i += 1\n",
    "\n",
    "    #print(noStr[23:26]) # pulling from the orginal data to identify the 2 values we need to solve for < t_n -> t_n-2 >   \n",
    "    moving_averages.append(noStr[24]) # First assign f(t_24) from no\n",
    "    moving_averages.append(noStr[25])\n",
    "    moving_averages[24] = round((moving_averages[23]+moving_averages[25])/2,2)\n",
    "    return(np.asarray(moving_averages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f9e8cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.67 3.83 3.9  4.13 4.13 4.3  4.23 4.17 4.1  4.3  4.33 4.27 4.2  4.2\n",
      " 4.1  4.03 4.03 4.2  4.13 3.9  3.77 3.83 3.97 4.   4.   4.  ]\n"
     ]
    }
   ],
   "source": [
    "window_avgs = moving_average(dataArr[1])\n",
    "print(window_avgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b396284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.           3.87847222   4.29097222   5.36180556  49.64722222\n",
      "  68.54930556  90.55972222 118.62083333 166.54652778 188.58819444\n",
      " 209.55972222 223.58055556 244.54722222 258.51666667 272.61041667\n",
      " 300.59930556 328.66111111 393.64444444 439.57916667 481.43819444\n",
      " 509.55069444 537.55972222 579.54444444 607.52361111 705.61111111\n",
      " 845.65069444]\n"
     ]
    }
   ],
   "source": [
    "print(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efe2b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d42d4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[145.0, 145.33, 143.33, 141.67, 140.67, 139.33, 139.67, 139.0, 139.67, 139.33, 140.33, 141.33, 141.0, 141.0, 140.0, 141.0, 140.67, 140.67, 140.33, 139.33, 139.33, 138.33, 139.0, 139.67, 140.33, 141.0]\n"
     ]
    }
   ],
   "source": [
    "print(slidingData[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b49b09e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.6700],\n",
      "        [3.8300],\n",
      "        [3.9000],\n",
      "        [4.1300],\n",
      "        [4.1300],\n",
      "        [4.3000],\n",
      "        [4.2300],\n",
      "        [4.1700],\n",
      "        [4.1000],\n",
      "        [4.3000],\n",
      "        [4.3300],\n",
      "        [4.2700],\n",
      "        [4.2000],\n",
      "        [4.2000],\n",
      "        [4.1000],\n",
      "        [4.0300],\n",
      "        [4.0300],\n",
      "        [4.2000],\n",
      "        [4.1300],\n",
      "        [3.9000],\n",
      "        [3.7700],\n",
      "        [3.8300],\n",
      "        [3.9700],\n",
      "        [4.0000],\n",
      "        [4.0000],\n",
      "        [4.0000]])\n"
     ]
    }
   ],
   "source": [
    "true_y = torch.tensor(window_avgs.copy().astype('float32')).reshape((-1,1)).to(device)\n",
    "t = torch.tensor(np.flip(times/24/60).copy()).to(device)\n",
    "print(true_y)\n",
    "best_loss = 1000000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "500cae97",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Learnable ML data \n",
    "\n",
    "def create_dataset(dataset, time_set, batch_time, len):\n",
    "    dataY0, dataY , time_T = [], [], []\n",
    "    for i in range(len-batch_time):\n",
    "        dataY0.append(dataset[i]) #Initail values for ODE\n",
    "        dataY.append(dataset[i:(i+batch_time)]) #predict batch_time, steps/label ahead from initial\n",
    "        time_T.append(time_set[i:(i+batch_time)])\n",
    "    return dataY0, dataY, time_T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f1ac6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 1])\n",
      "torch.Size([15, 5, 1])\n",
      "torch.Size([15, 5])\n"
     ]
    }
   ],
   "source": [
    "#Training \n",
    "train_y0, train_y, train_t = create_dataset(dataset = true_y[:train_len], time_set = t[:train_len], batch_time = batch_time , len = train_len)\n",
    "train_y0 = torch.stack(train_y0)  # (M, 1, D)\n",
    "train_y = torch.stack(train_y) # (M, T, 1, D)\n",
    "train_t = torch.stack(train_t) # (M, T)\n",
    "print(train_y0.shape)\n",
    "print(train_y.shape)\n",
    "print(train_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cfcafd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1])\n",
      "torch.Size([1, 5, 1])\n",
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "test_y0, test_y, test_t = create_dataset(dataset=true_y[train_len:(train_len+test_len)], time_set=t[train_len:(train_len+test_len)],\n",
    "                                              batch_time=batch_time, len=test_len)\n",
    "test_y0 = torch.stack(test_y0)\n",
    "test_y = torch.stack(test_y)\n",
    "test_t = torch.stack(test_t) \n",
    "print(test_y0.shape)\n",
    "print(test_y.shape)\n",
    "print(test_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd268cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ Put data and labels together via TensorDataset -------------\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(train_y0, train_y, train_t)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = torch.utils.data.TensorDataset(test_y0, test_t, test_t)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fdcd5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79de83c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abe8e1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODEFunc(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ODEFunc, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim1, hidden_dim2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim2, output_dim)\n",
    "        )\n",
    "\n",
    "        for m in self.net.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.1)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "    def forward(self, t, y):\n",
    "        y = torch.squeeze(y)\n",
    "        y_new = torch.hstack((t, y))\n",
    "        y_new = y_new[None, None, :]\n",
    "        #print(f'new y is: {y_new}, {y_new.shape}')\n",
    "        return self.net(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5c4a980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 5, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52e68e43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time is 158.087060213089\n"
     ]
    }
   ],
   "source": [
    "ii = 0  #\n",
    "func = ODEFunc().to(device)  # RHS of ODE system parameterized by FNN\n",
    "optimizer = optim.Adam(func.parameters(), lr=learning_rate)\n",
    "start = time.time()\n",
    "best_loss = 10000000\n",
    "train_loss_vector = []   # batch loss on training set\n",
    "test_loss_vector = []    # total loss on test set\n",
    "best_iter = 0\n",
    "x=0\n",
    "y=0\n",
    "\n",
    "for itr in range(1, niters + 1):  # iteration starts from 1\n",
    "    test_loss = 0.0\n",
    "    train_loss = 0.0\n",
    "    for batch_idx, (y0_train, y_train, t_train) in enumerate(train_loader):  # mini-batch\n",
    "        batch_loss = 0.0\n",
    "        for i in range(len(t_train)):  # feed the samples from mini-batch to odeint one by one\n",
    "            # print(f'------ batch idx {batch_idx}, i is: {i}, len of t is: {len(t_train)}')\n",
    "            y_train1 = y_train[i]  # pick one sample with shape (T, 1, D)\n",
    "            y_train1 = y_train1[:, None, None, :]  # convert shape of y from (T, 1, D) to ideal shape (T, 1, 1, D)\n",
    "            y0_train1 = y0_train[i]  # pick one initial value with shape (1, D)\n",
    "            y0_train1 = y0_train1[None, :]  # convert shape of y0 from (1, D) to ideal shape (1, 1, D)\n",
    "            pred_y = odeint(func, y0_train1, t_train[i]).to(device)  # shape of t_train is (batch_time, )\n",
    "            loss = torch.mean(torch.square(pred_y - y_train1))\n",
    "            batch_loss += loss\n",
    "        optimizer.zero_grad()   # zero all your gradient before you do the backpro.\n",
    "        batch_loss.backward()   # backpro to compute the gradient\n",
    "        optimizer.step()  # update the parameters theta=(W,b) using the gradient obtained using loss.backward()\n",
    "        train_loss += batch_loss.item()  # count training loss for all training data points\n",
    "        train_loss_vector.append(train_loss)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx1, (y0_test, y_test, t_test) in enumerate(test_loader):  # mini-batch\n",
    "            batch_loss1 = 0.0\n",
    "            for i in range(len(t_test)):  # feed samples from mini-batch to odeint one by one\n",
    "                y_test1 = y_test[i]  # pick one element with shape (T, 1, D)\n",
    "                #y_test1 = y_test1[:, None, 1, :]  # convert the shape from (T, 1, D) to (T, 1, 1, D)\n",
    "                y0_test1 = y0_test[i]  # pick one initial with shape (1, D)\n",
    "                y0_test1 = y0_test1[None, :]   # convert the shape to (1, 1, D)\n",
    "                pred_y1 = odeint(func, y0_test1, t_test[i])\n",
    "                #print(pred_y)\n",
    "                loss1 = torch.mean(torch.square(pred_y1 - y_test1))\n",
    "                batch_loss1 += loss1.item()\n",
    "            test_loss += batch_loss1\n",
    "            test_loss_vector.append(test_loss)\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            best_iter = itr\n",
    "            save_path = f'/Users/dezmon/CSCE587/REU/best_model_{batch_time}_{niters}.pth'\n",
    "            torch.save(func.state_dict(), save_path)  # saving the model\n",
    "            \n",
    "end = time.time()\n",
    "print(f'training time is {end-start}')\n",
    "_bt = batch_time\n",
    "_bs = batch_size\n",
    "while(os.path.exists(f'/Users/dezmon/CSCE587/REU/training loss_{_bt}_{_bs}.txt')):\n",
    "    _bt += 1\n",
    "    _bs += 1\n",
    "                \n",
    "#np.savetxt(f'training loss_{batch_time}_{batch_size}.txt', train_loss_vector, delimiter='\\n')\n",
    "#np.savetxt(f'val loss_{batch_time}_{batch_size}.txt', test_loss_vector, delimiter='\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e93908b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dy/3wcl9s793cl8pnn1nlp2tj4w0000gn/T/ipykernel_81273/1661862261.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ----------- Visualization --------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m func.load_state_dict(\n\u001b[0;32m----> 3\u001b[0;31m     torch.load(f'/Users/dezmon/CSCE587/REU/best_model_/best_model_{x}_{y}.pth', map_location=lambda storage, loc: storage))\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# -------- short term prediction ------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstart_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# ----------- Visualization --------------\n",
    "func.load_state_dict(\n",
    "    torch.load(f'/Users/dezmon/CSCE587/REU/best_model_/best_model_{x}_{y}.pth', map_location=lambda storage, loc: storage))\n",
    "# -------- short term prediction ------------\n",
    "start_point = 0\n",
    "end_point = 2\n",
    "l = 0\n",
    "D = 1  # dim of bio-markers\n",
    "y_pred = np.zeros((data_size, D))\n",
    "N = np.floor(data_size/end_point)  # - 1\n",
    "for i in range(int(N)):\n",
    "    a = true_y[start_point]  # pick up the initial value\n",
    "    a = a[None, :]  # convert dim to (1, 1, D)\n",
    "    t0 = t[start_point:start_point + end_point]  # (T, )\n",
    "    b = odeint(func, a, t0)  # shape (T, 1, 1, D)\n",
    "    b = b.detach().cpu().numpy()\n",
    "    b = np.reshape(b, (end_point, D))  # shape (T, D) with T=end_point\n",
    "    print(f'i {i} and start {start_point}')\n",
    "    y_pred[start_point: start_point+end_point] = b\n",
    "    start_point += end_point\n",
    "\n",
    "np.savetxt(f'final prediction_{end_point}-steps.txt', y_pred, delimiter='\\n')\n",
    "plt.plot(t.cpu(), true_y[:, 0].cpu(), '-o')\n",
    "plt.plot(t.cpu(), y_pred[:, 0], '-*')\n",
    "plt.xlabel('t')\n",
    "plt.ylabel('y')\n",
    "plt.legend(['ground truth', 'prediction'])\n",
    "plt.title(f'pred_{end_point}-steps_{batch_time}_{niters}')\n",
    "plt.show()\n",
    "\n",
    "# Relative Error\n",
    "relative_error = torch.sum(torch.abs(torch.tensor(y_pred[:,0]) - true_y[:, 0].cpu()) / true_y[:, 0].cpu())\n",
    "print(f'Relative Error: {relative_error.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e6efb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9070a3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905da35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95470f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
